{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Distributed with Tensorflow Spark 3 Reading FS\n",
    "\n",
    "Exemplo de identificação de números com a base MNIST usando spark_tensorflow_distributor lendo imagens no filesystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "import tensorflow as tf\n",
    "from spark_tensorflow_distributor import MirroredStrategyRunner\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib, getpass, shutil, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "CATEGORIES = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "# Set Small MNIST Dataset\n",
    "DATADIR = \"/data/dataset/mnist/trainingSample/trainingSample\"\n",
    "TESTDIR = \"/data/dataset/mnist/testSample/testSample\"\n",
    "\n",
    "# Set Big MNIST Dataset\n",
    "#DATADIR = \"/data/dataset/mnist/trainingSet/trainingSet\"\n",
    "#TESTDIR = \"/data/dataset/mnist/trainingSample/trainingSample\"\n",
    "\n",
    "data_train = DATADIR\n",
    "data_test = TESTDIR\n",
    "#data_train = pathlib.Path(DATADIR)\n",
    "#data_test = pathlib.Path(TESTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  600\n",
      "Number of test images:  350\n"
     ]
    }
   ],
   "source": [
    "SIZE_OF_DATASET = len(list(pathlib.Path(DATADIR).glob('*/*.jpg')))\n",
    "SIZE_OF_TEST = len(list(pathlib.Path(TESTDIR).glob('*/*.jpg')))\n",
    "\n",
    "print(\"Number of training images: \",SIZE_OF_DATASET)\n",
    "print(\"Number of test images: \",SIZE_OF_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set auxiliar directories\n",
    "base_dir = '/data/users/tensorflow/' + getpass.getuser()\n",
    "mnist_model = base_dir + '/model'\n",
    "mnist_export = base_dir + '/export'\n",
    "mnist_logs = base_dir + '/logs'\n",
    "\n",
    "# Clean log dir\n",
    "shutil.rmtree(mnist_logs, ignore_errors=True, onerror=None)\n",
    "os.makedirs(mnist_logs, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8\n",
    "IMG_SIZE = 28\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Spark Cluster with Docker Tensorflow\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"MNIST spark_tensorflow_distributor with Mesos\") \\\n",
    "        .master(\"mesos://zk://10.129.64.20:2181,10.129.64.10:2181,10.129.64.30:2181/mesos\") \\\n",
    "        .config(\"spark.mesos.executor.docker.image\",\"lasid/spark-worker-tensorflow:3.1.1_bionic\") \\\n",
    "        .config(\"spark.mesos.containerizer\",\"docker\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Run local\n",
    "#        .appName(\"MNIST spark_tensorflow_distributor local execution\") \\\n",
    "#         .master(\"local[*]\") \\\n",
    "\n",
    "# run with Mesos\n",
    "#        .master(\"mesos://zk://10.129.64.20:2181,10.129.64.10:2181,10.129.64.30:2181/mesos\") \\\n",
    "#        .config(\"spark.mesos.executor.docker.image\",\"lasid/spark-worker-tensorflow:3.1.1_bionic\") \\\n",
    "#        .config(\"spark.mesos.containerizer\",\"docker\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main train function\n",
    "\n",
    "def train():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os, random, cv2\n",
    "    \n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    def prep_data(DATA_DIR, CATEGORIES):\n",
    "        data = []\n",
    "        for category in CATEGORIES:\n",
    "            path = os.path.join(DATA_DIR,category)\n",
    "            class_num = CATEGORIES.index(category)\n",
    "            for img in os.listdir(path):\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                data.append([new_array, class_num])\n",
    "            #plt.figure(figsize=(1,1))\n",
    "            #plt.imshow(new_array, cmap='gray')\n",
    "            #plt.show()\n",
    "        return data\n",
    "    \n",
    "    def prep_2(data):\n",
    "        random.shuffle(data)\n",
    "        X = []\n",
    "        y = []\n",
    "        for features,label in data:\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "        res = np.eye(10)[y]\n",
    "        X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        return X, res\n",
    "\n",
    "    def build_and_compile_cnn_model():\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "        #model.compile(\n",
    "        #    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        #    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "        #    metrics=['accuracy'])\n",
    "        return model\n",
    " \n",
    "    data = prep_data(TESTDIR, CATEGORIES)\n",
    "    tX, ty = prep_2(data)\n",
    "    data2 = prep_data(DATADIR, CATEGORIES)\n",
    "    X, y = prep_2(data2)\n",
    "    X=np.array(X/255.0)\n",
    "    y=np.array(y)\n",
    "    tX=np.array(tX/255.0)\n",
    "    ty=np.array(ty)\n",
    "\n",
    " #   options = tf.data.Options()\n",
    " #   options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    " #   train_datasets = train_datasets.with_options(options)\n",
    "    multi_worker_model = build_and_compile_cnn_model()\n",
    "    multi_worker_model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(tX, ty))  #, callbacks=callbacks)\n",
    "#    np.save(mnist_logs+'/my_history.npy',multi_worker_model.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MirroredStrategyRunner parameters\n",
    "\n",
    "### API default\n",
    "MirroredStrategyRunner(num_slots, local_mode=False, use_gpu=True, gpu_resource_name='gpu', use_custom_strategy=False)\n",
    "\n",
    "### Args\n",
    "\n",
    "1. num_slots: Total number of GPUs or CPU only Spark tasks that participate in distributed training.\n",
    "\n",
    "2. local_mode: If True, the training function will be run locally on the driver. If False training is distributed among the workers.\n",
    "\n",
    "3. use_gpu: If True, training is done with GPUs using Spark resource scheduling with the gpu_resource_name parameter as the resource name. If False, do CPU only training.\n",
    "\n",
    "4. gpu_resource_name: The name of the Spark resource scheduling GPU resource.\n",
    "\n",
    "5. use_custom_strategy: When true, the training function passed to the MirroredStrategyRunner.run method must construct and use its own tensorflow.distribute.Strategy() object. When false, MirroredStrategyRunner constructs one for the user allowing the user to provide non-distributed TensorFlow code that is executed as distributed code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doing CPU training...\n",
      "Will run with 7 Spark tasks.\n",
      "Distributed training in progress...\n",
      "View Spark executor stderr logs to inspect training...\n",
      "Training with 7 slots is complete!\n"
     ]
    }
   ],
   "source": [
    "# Start cluster...\n",
    "MirroredStrategyRunner(num_slots=7, use_gpu=False).run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=np.load(mnist_logs+'/my_history.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "#plt.plot(history.history['accuracy'], label='accuracy')\n",
    "#plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.ylim([0.5, 1])\n",
    "#plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
